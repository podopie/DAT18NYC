{
 "metadata": {
  "name": "",
  "signature": "sha256:f9fcfcc25b32dfa4f5eb81a1f746e3fe8e4e3520a5ac5558f538497b1510d54a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%html\n",
      "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x10438cd50>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Matrix Algebra and the Regression Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Today we'll review how matrix algebra solves for the ordinary least squares regression, different cost functions for different linear solutions, and how we can use a linear solution to solve for probability of a given class."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Understand how Matrix Algebra is used to find coefficients in an ordinary least squares regression\n",
      "* Review two techniques to improving the linear model\n",
      "* Understanding the use case and results of a Logistic Regression\n",
      "* New Cross Validation Technique - **K-Fold**\n",
      "* First Metric for Classification: **Accuracy**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code Dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "library | function | explanation\n",
      "--------|----------|------------\n",
      "`statsmodels` | `fit_regularlized` | Includes regularization while solving for the model\n",
      "`sklearn.linear_model` | `LogisticRegression` | The logistic regression class in sklearn. It's built using the library [liblinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)\n",
      "`sklearn.cross_validation` | `KFold` | generates K train test splits given a data set; data is not reused the sets that return include the index values of a train and test set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Matrix Algebra Review\n",
      "\n",
      "In order to best understand most machine learning algorithms, we need some basis of linear algebra.\n",
      "\n",
      "> Linear algebra is best defined as mathematics in the multidimensional space and the mapping between said spaces.\n",
      "\n",
      "#### For Example\n",
      "\n",
      "$$y = mx + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3} + m_{4}x_{4} + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3} + m_{4}x_{4} + m_{5}x_{5} + m_{6}x_{6} + m_{7}x_{7} + m_{8}x_{8} + m_{9}x_{9} + m_{10}x_{10} + b$$\n",
      "\n",
      "### Matrices\n",
      "\n",
      "> Matrices are an array of real numbers with m rows and n columns\n",
      "\n",
      "Each value in a matrix is called an entry.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 5 & 8 & 7\\\\2 & 1 & 3 & 6\\\\3 & 5 & 1 & 0\\\\4 & 6 & 0 & 1\\end{bmatrix}$$\n",
      "\n",
      "$A_{2,1}$ in the given matrix, refers to the entry on the 2nd row, in the 1st column. The value is 2. \n",
      "\n",
      "\n",
      "### Vectors\n",
      "\n",
      "> Vectors are a special kind of matrix, as they only consist of one dimension of real numbers.\n",
      "\n",
      "These look most like a numeric array (or **list**) in Python.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix}$$\n",
      "\n",
      "Likewise, you can refer to each index or value similarly (`a[0]` in Python is the same entity as 0 in vector a)\n",
      "\n",
      "\n",
      "### Properties of Arrays\n",
      "\n",
      "**Rule 1**: Matrices can be added together only when they are the same size. If they are not the same size, their sum is **undefined**.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix} + \\begin{bmatrix}2 & 5 & 9 & 4\\end{bmatrix} = \n",
      "\\begin{bmatrix}3 & 8 & 18 & 6\\end{bmatrix} $$\n",
      "\n",
      "$$\\begin{bmatrix}8 & 72 & 3 & 1\\end{bmatrix} + \\begin{bmatrix}17 & 55 & 3 & 10\\end{bmatrix} = \n",
      " \\ ? $$\n",
      " \n",
      " \n",
      "**Rule 2**: Matrices can be multiplied by a scalar (single entity) value.\n",
      "\n",
      "> The result will always be the shape of the matrix.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix} * 3 =\n",
      "\\begin{bmatrix}3 & 9 & 27 & 6\\end{bmatrix} $$\n",
      "\n",
      "$$\\begin{bmatrix}8 & 72 & 3 & 1\\end{bmatrix} * 2 = \\ ? $$\n",
      "\n",
      "\n",
      "**Rule 3**: Matrices and vectors can be multiplied together given that the matrix columns are as wide as the vector is long. \n",
      "\n",
      "> The result will always be a vector.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\\\2 & 4 & 6 & 8\\end{bmatrix} * \\begin{bmatrix}2 \\\\ 3 \\\\ 6 \\\\ 5  \\end{bmatrix} = \\begin{matrix} 2 + 9 + 54 + 10 \\\\ 4 + 12 + 36 + 40 \\end{matrix} = \\begin{bmatrix} 75 \\\\ 92 \\end{bmatrix}$$\n",
      "\n",
      "**Rule 4**: Matrices can be multiplied together using the same rules that we have from matrix-vector multiplication.\n",
      "\n",
      "> The result will always be a matrix.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\\\2 & 4 & 6 & 8\\end{bmatrix} * \\begin{bmatrix}2 & 1 \\\\ 3 & 2 \\\\ 6 & 0 \\\\ 5 & 4 \\end{bmatrix} = \\begin{matrix} 2 + 9 + 54 + 10 \\\\ 4 + 12 + 36 + 40 \\\\ 1 + 6 + 0 + 8 \\\\ 2 + 8 + 0 + 32\\end{matrix} = \\begin{bmatrix} 75 & 15 \\\\ 92 & 42 \\end{bmatrix}$$\n",
      "\n",
      "\n",
      "**Rule 5**: Solving for the inverse of a 2x2 matrix is equal to 1 over the determinant of A (scaler) times $\\begin{bmatrix}d & -b\\\\-c & a\\end{bmatrix}$\n",
      "\n",
      "> The result will be the same shape as the original matrix\n",
      "\n",
      "$$det(A) = ad - bc$$\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3\\\\2 & 4\\end{bmatrix}^{-1} = \\frac{1}{1*4 - 3*2} * \\begin{bmatrix}4 & -3\\\\-2 & 1\\end{bmatrix} = \\begin{bmatrix}-2 & 1.5\\\\1 & -0.5\\end{bmatrix}$$\n",
      "\n",
      "### Why This Matters\n",
      "\n",
      "Matrices represent the multiple dimensions in our data! If we had a vector that suggested how important each dimension of our data was, we could use that to find our best **linear model**.\n",
      "\n",
      "\n",
      "### The Ordinary Least Squares (OLS) Linear Regression Formula\n",
      "\n",
      "A regression model is a functional relationship between input & response\n",
      "variables.\n",
      "\n",
      "The **simple linear regression** model captures a linear relationship between a single input variable x and a response variable y: \n",
      "\n",
      "$$y = \u03b1+\u03b2x+\u03b5$$\n",
      "\n",
      "* $y$ = response variable (the one we want to predict)\n",
      "* $x$ = input variable (the one we use to train the model)\n",
      "* $\u03b1$ = intercept (where the line crosses the y-axis)\n",
      "* $\u03b2$ = regression coefficient (the model \u201cparameter\u201d)\n",
      "* $\u03b5$ = residual (the prediction error)\n",
      "\n",
      "The **cost function** or goal of the ordinary least squares regression is to find the linear solution with the least sum of square error.\n",
      "\n",
      "#### Solving for OLS\n",
      "\n",
      "We'll break down the math here:\n",
      "\n",
      "The OLS Linear Regression is just matrix algebra (the stuff from up above!)\n",
      "\n",
      "Let\u2019s go over the math by hand so we can understand how we determine the regression coefficient.\n",
      "\n",
      "A linear regression in its simplest form:\n",
      "\n",
      "$y = \\alpha + \\beta x + \\epsilon$\n",
      "\n",
      "but we can assume that our $\\alpha$ (y-itercept)is either 0 or 1, and $\\epsilon$ (error) is zero.\n",
      "\n",
      "$y = \u03b2x$\n",
      "\n",
      "but we want to solve for \u03b2, which means our new equation now looks like this:\n",
      "\n",
      "$\u03b2 = ( X^TX)^{-1} X^Ty$\n",
      "\n",
      "\n",
      "#### How did we get there?\n",
      "\n",
      "The below is problematic, as we cannot divide by a matrix! So we first square the matrix.\n",
      "\n",
      "$\u03b2 = \\frac{y}x == \\frac{xy}{x^2}$\n",
      "\n",
      "Recall in algebra:\n",
      "\n",
      "$\\frac{1}x = x^{-1}$\n",
      "\n",
      "Inverting the matrix since raising $x$ to the power of negative 1 is equal to $1$ over $x$\n",
      "\n",
      "$\\frac{1}{x{^2}} * \\frac{xy}1$\n",
      "\n",
      "$(XX)^{-1}XY$\n",
      "\n",
      "And finally to make it programmer friendly:\n",
      "\n",
      "$\u03b2 = ( X^TX)^{-1} X^TY$\n",
      "\n",
      "#### Example\n",
      "\n",
      "So if we had data:\n",
      "\n",
      "    Input  Output\n",
      "    3.385  44.5\n",
      "    0.48   15.5\n",
      "    1.35   8.1\n",
      "    465    423\n",
      "    36.33  119.5\n",
      "\n",
      "$$\u03b2=\\left(\n",
      "    \\begin{array}{r}\n",
      "         \\begin{matrix}\n",
      "             1 & 1 & 1 & 1 & 1 & \\\\\n",
      "             3.385 & 0.48 & 1.35 & 465 & 36.33\n",
      "         \\end{matrix}\n",
      "         \\begin{matrix}\n",
      "            1 & 3.385 & \\\\\n",
      "            1 & 0.48 & \\\\\n",
      "            1 & 1.35 & \\\\\n",
      "            1 & 465 & \\\\\n",
      "            1 & 36.33\n",
      "         \\end{matrix}\n",
      "    \\end{array}\n",
      "  \\right)^{-1}\n",
      "  \\cdots\n",
      "$$\n",
      "  \n",
      "$$\u03b2 = ( X^TX)^{-1} \\cdots$$\n",
      "\n",
      "$$\\cdots\n",
      "    \\left(\n",
      "    \\begin{array}{r}\n",
      "         \\begin{matrix}\n",
      "             1 & 1 & 1 & 1 & 1 & \\\\\n",
      "             3.385 & 0.48 & 1.35 & 465 & 36.33\n",
      "         \\end{matrix}\n",
      "         \\begin{matrix}\n",
      "            44.5 & \\\\\n",
      "            15.5 & \\\\\n",
      "            8.1 & \\\\\n",
      "            423 & \\\\\n",
      "            119.5\n",
      "         \\end{matrix}\n",
      "    \\end{array}\n",
      "  \\right)\n",
      "$$\n",
      "  \n",
      "$$ \\cdots X^TY$$\n",
      "\n",
      "$$\u03b2=\\begin{array}{r}\n",
      "         \\begin{bmatrix}\n",
      "            0.2617 & -0.0006 & \\\\\n",
      "            -0.0006 & 0.000006\n",
      "         \\end{bmatrix}\n",
      "         \\begin{bmatrix}\n",
      "            610.6 & \\\\\n",
      "            201205.4425\n",
      "         \\end{bmatrix}\n",
      "    \\end{array}\n",
      "$$\n",
      "\n",
      "$$\u03b2 = ( X^TX)^{-1} X^TY$$\n",
      "\n",
      "$$ \\begin{bmatrix}\n",
      "            37.2 & \\\\\n",
      "            0.838\n",
      "         \\end{bmatrix}\n",
      "    = \\begin{array}{r}\n",
      "         \\begin{bmatrix}\n",
      "            0.2617 & -0.0006 & \\\\\n",
      "            -0.0006 & 0.000006\n",
      "         \\end{bmatrix}\n",
      "         \\begin{bmatrix}\n",
      "            610.6 & \\\\\n",
      "            201205.4425\n",
      "         \\end{bmatrix}\n",
      "    \\end{array}\n",
      "$$\n",
      "\n",
      "$$\u03b2 = ( X^TX)^{-1} X^TY$$\n",
      "\n",
      "<div style='text-align: center;'>(with some rounding...)</div>\n",
      "\n",
      "$$ \\begin{array}{c}\n",
      "            Intercept : 37.2 & \\\\\n",
      "            \u03b2 : 0.838\n",
      "         \\end{array}\n",
      "$$\n",
      "\n",
      "#### Evaluate and verify in Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import linear_model as lm\n",
      "\n",
      "practice_set = pd.DataFrame({\n",
      "    'x': [3.385, 0.48, 1.35, 465, 36.33],\n",
      "    'y': [44.5, 15.5, 8.1, 423, 119.5],\n",
      "})\n",
      "\n",
      "A = np.array([[1, 1, 1, 1, 1], practice_set['x']])\n",
      "# print np.linalg.inv(A.dot(A.T)).dot(A.dot(practice_set['y']))\n",
      "\n",
      "model = lm.LinearRegression().fit(practice_set[['x']], practice_set['y'])\n",
      "#print model.intercept_\n",
      "#print model.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Other cost techniques: Regularization\n",
      "\n",
      "Often times we do **not** want to solve with the OLS regression, because it may be accounting for *bias* in the data set, and not necessarily the *variance* within the data:\n",
      "\n",
      "* **Bias** refers to predictions that are systematically\n",
      "inaccurate. \n",
      "* **Variance** refers to predictions that are generally\n",
      "inaccurate.\n",
      "\n",
      "Therefore, it may make sense to adjust the cost function of our regression to trade fitting to the bias of the data set, so we may have a **less accurate model**, but a **better general, more applicable model**. Recall: our goal is not to find the best fit, but to find the fit that best explains the variance in our data.\n",
      "\n",
      "We define these with two **regularization** techniques:\n",
      "\n",
      "#### L1 regularization: Used when we have small data but many features.\n",
      "\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i \\rVert \\lt \u03bb$\n",
      "    \n",
      "#### L2 regularization: Used in just about all other cases.\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert^2)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert^2)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i^2 \\rVert \\lt \u03bb$\n",
      "\n",
      "\n",
      "#### What this looks like in our code: exploring other regressions in sklearn\n",
      "\n",
      "The linear_model module in sklearn is actually for regularized regressions. Each that include a regulization technique include a hyperparameter `alpha` to set--this would be the $\\lambda$ from above. A quick comparison:\n",
      "\n",
      "regression | class | L1? | L2?\n",
      "-----------|-------|:-------------:|:------------:\n",
      "Ordinary Least Squares | `LinearRegression()` | - | -\n",
      "Ridge Regression | `Ridge()` | - | $\\checkmark$ \n",
      "Lasso Regression | `Lasso()` | $\\checkmark$ | - \n",
      "Elastic Net | `ElasticNet()` | $\\checkmark$ | $\\checkmark$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Other cost techniques: Regularization\n",
      "\n",
      "Often times we do **not** want to solve with the OLS regression, because it may be accounting for *bias* in the data set, and not necessarily the *variance* within the data:\n",
      "\n",
      "* **Bias** refers to predictions that are systematically\n",
      "inaccurate. \n",
      "* **Variance** refers to predictions that are generally\n",
      "inaccurate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='img/darts.jpg' style='float: left; height: 300px;' />\n",
      "<img src='img/biasvariance.png' style='float: left;  height: 300px;' />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [http://www.kdnuggets.com/2012/09/pedro-domingos-useful-things-about-machine-learning.html](http://www.kdnuggets.com/2012/09/pedro-domingos-useful-things-about-machine-learning.html)\n",
      "2. [http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, it may make sense to adjust the cost function of our regression to trade fitting to the bias of the data set, so we may have a **less accurate model**, but a **better general, more applicable model**. Recall: our goal is not to find the best fit, but to find the fit that best explains the variance in our data.\n",
      "\n",
      "We define these with two **regularization** techniques:\n",
      "\n",
      "#### L1 regularization: Used when we have small data but many features.\n",
      "\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i \\rVert \\lt \u03bb$\n",
      "    \n",
      "#### L2 regularization: Used in just about all other cases.\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert^2)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert^2)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i^2 \\rVert \\lt \u03bb$\n",
      "\n",
      "\n",
      "#### What this looks like in our code: exploring other regressions in sklearn\n",
      "\n",
      "The linear_model module in sklearn is actually for regularized regressions. Each that include a regulization technique include a hyperparameter `alpha` to set--this would be the $\\lambda$ from above. A quick comparison:\n",
      "\n",
      "regression | class | L1? | L2?\n",
      "-----------|-------|:-------------:|:------------:\n",
      "Ordinary Least Squares | `LinearRegression()` | - | -\n",
      "Ridge Regression | `Ridge()` | - | $\\checkmark$ \n",
      "Lasso Regression | `Lasso()` | $\\checkmark$ | - \n",
      "Elastic Net | `ElasticNet()` | $\\checkmark$ | $\\checkmark$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Try it: Fitting different models\n",
      "\n",
      "Start with the following base model, generated from a previous class:\n",
      "\n",
      "```python\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "boston = load_boston()\n",
      "desc = boston.DESCR\n",
      "bostondf = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
      "y_col = 'MEDV'\n",
      "bostondf[y_col] = boston.target\n",
      "x_cols = [\n",
      "    'CRIM',\n",
      "    'ZN',\n",
      "    'INDUS',\n",
      "    'CHAS',\n",
      "    'NOX',\n",
      "    'RM',\n",
      "    'AGE',\n",
      "    'DIS',\n",
      "    'RAD',\n",
      "    'TAX',\n",
      "    'PTRATIO',\n",
      "    'B',\n",
      "    'LSTAT',\n",
      "]\n",
      "\n",
      "model = lm.LinearRegression().fit(bostondf[x_cols], bostondf[y_col])\n",
      "```\n",
      "\n",
      "Small groups will work on only **one** of the following questions (do as assigned)\n",
      "\n",
      "1. Using one of the `Ridge()`, `Lasso()`, or `ElasticNet()` classes, write a for loop around that inserts a value for alpha, fits the model, and stores the Mean Square Error (Mean Square Error is in the metrics module of sklearn). Plot a line chart where x is the alpha parameter and y is the mean squared error. What does the line chart look like? (Use alphas between 0.1 and 0.000000000001)\n",
      "2. Using one of the `Ridge()`, `Lasso()`, or `ElasticNet()` classes, write a for loop around that inserts a value for alpha, fits the model, and stores the `predict()` results from the model into a DataFrame. Plot a line chart where x is the one of the X columns and y is the predicted value. What does the line chart look like? (Use alphas between 0.1 and 0.000000000001)\n",
      "3. Using each Regression class (all four from the above table), run a test train split, calculate the difference in $R^2$ between the test and train, and plot each result (x labels being the regression used, y being the difference in $R^2$ between test and train) Use the default alpha parameter. Which regression had the the most consistent result?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Logistic function\n",
      "\n",
      "Logistic Regression is an implementation of the linear regression often used to solve a **binary** problem (though it can be used to do others). What sets it apart from linear discriminant models (say, a line that separates two classes) is that it provides _probabilities_ of a given class, as opposed to assigning a class. Since most classification problems are binary, the results are interpretive, much like a linear regression.\n",
      "\n",
      "Class probability is generally represented in a space of $0$ to $inf$. To use a linear model to determine probality of a given class, we want to use a function that gives us results from $-inf$ to $inf$. Therefore, to get to this mode, we use log, and work with log odds.\n",
      "\n",
      "One challenge is that linear models (OLS, logistic, etc) often expect some type of normal distribution. \n",
      "\n",
      "#### Advantages of the Logistic Regression\n",
      "* Interpretable (coefficients; weights)\n",
      "* Parameters are few : increase linearly with dimensionality\n",
      "* Extensible to multi-class\n",
      "\n",
      "#### Equation\n",
      " $P(Y=1) = \\dfrac{e^{(\\alpha + \\beta x)}}{1 + e^{(\\alpha + \\beta x)}}$\n",
      "\n",
      "#### Coefficient Estimation: Maximum Likelihood Estimation\n",
      "* Which values of the coefficients make the observed data most likely to have occurred?\n",
      "* Take the Beta and raise $10^\\beta$ for odds ratio\n",
      "\n",
      "#### Odds, Log-odds\n",
      "\n",
      "The coefficients of the logistic regression represent the log-odds of the target given that feature. We'll need to use the `exp()` function to transform them into more human readable odds ratios."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='img/bernoulli.png' />"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Statsmodels, because we love statsmodels,\n",
      "### and because logistic regressions are still common for data exploration:from sklearn import datasets.\n",
      "### Since logistic is used to solve binomial problems, our targets should be 0 and 1.\n",
      "### by default, fit_regularized uses L1 Regularization\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import statsmodels.formula.api as smf\n",
      "iris = datasets.load_iris()\n",
      "irisdf = pd.DataFrame(iris.data, columns=['sep_len', 'sep_wid', 'pet_len', 'pet_wid'])\n",
      "irisdf['target'] = iris.target\n",
      "irisdf = irisdf.query('target in (1, 2)')\n",
      "irisdf['target'] = irisdf['target'] - 1\n",
      "\n",
      "lmf = smf.logit('target ~ sep_wid + pet_wid', irisdf)\n",
      "# setting alpha to 0 effectively removes the hyperparameter\n",
      "results = lmf.fit_regularized(alpha=0)\n",
      "print results.summary()\n",
      "print np.exp(results.params)\n",
      "\n",
      "# sklearn implementation\n",
      "# note that by default, LogisticRegression() uses L2 Regularization.\n",
      "# C, in this case, is the alpha parameter\n",
      "# we can't remove it (it's a fickle sklearn library issue), but we can make is huge so it as less of an effect.\n",
      "clf = lm.LogisticRegression(C=1e100).fit(irisdf[['sep_wid', 'pet_wid']], irisdf['target'])\n",
      "print clf.coef_\n",
      "print np.exp(clf.coef_)\n",
      "# same model, different regularization method\n",
      "clf2 = lm.LogisticRegression(penalty='l1', C=1e100).fit(irisdf[['sep_wid', 'pet_wid']], irisdf['target'])\n",
      "print clf2.coef_\n",
      "print np.exp(clf2.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.    (Exit mode 0)\n",
        "            Current function value: 0.13699530933\n",
        "            Iterations: 30\n",
        "            Function evaluations: 30\n",
        "            Gradient evaluations: 30\n",
        "                           Logit Regression Results                           \n",
        "==============================================================================\n",
        "Dep. Variable:                 target   No. Observations:                  100\n",
        "Model:                          Logit   Df Residuals:                       97\n",
        "Method:                           MLE   Df Model:                            2\n",
        "Date:                Wed, 25 Feb 2015   Pseudo R-squ.:                  0.8024\n",
        "Time:                        15:18:58   Log-Likelihood:                -13.700\n",
        "converged:                       True   LL-Null:                       -69.315\n",
        "                                        LLR p-value:                 7.025e-25\n",
        "==============================================================================\n",
        "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept    -14.3793      5.097     -2.821      0.005       -24.370    -4.389\n",
        "sep_wid       -3.9071      1.746     -2.237      0.025        -7.330    -0.485\n",
        "pet_wid       15.7003      3.662      4.288      0.000         8.523    22.877\n",
        "==============================================================================\n",
        "Intercept          0.000001\n",
        "sep_wid            0.020098\n",
        "pet_wid      6584846.949305\n",
        "dtype: float64\n",
        "[[ -3.90710181  15.70023074]]\n",
        "[[  2.00986665e-02   6.58451173e+06]]\n",
        "[[ -3.90529131  15.6941794 ]]\n",
        "[[  2.01350881e-02   6.54478694e+06]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross Validation: k-fold\n",
      "\n",
      "k-fold is splitting the dataset up evenly into a test and train split $k$ times. We want to determine:\n",
      "\n",
      "* do the cuts generally looks the same when generating a model?\n",
      "* should we consider the best model, or the average of k fits?\n",
      "\n",
      "sklearns implementation allows you to randomize before splitting the data. Consider the results here to be a *conservative* appoach to a data problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Metrics: Accuracy\n",
      "\n",
      "Accuracy measure number correctly predicted over total number of observations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.cross_validation\n",
      "\n",
      "kf = sklearn.cross_validation.KFold(n=len(irisdf), n_folds=3, shuffle=True, random_state=1234)\n",
      "\n",
      "for train, test in kf:\n",
      "    clf3 = lm.LogisticRegression(penalty='l1', C=1e100).fit(irisdf.iloc[train][['sep_wid', 'pet_wid']], irisdf.iloc[train]['target'])\n",
      "    # Score for most classifiers by default returns accuracy.\n",
      "    print clf3.coef_, clf3.score(irisdf.iloc[test][['sep_wid', 'pet_wid']], irisdf.iloc[test]['target'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ -4.48603278  14.43734254]] 0.911764705882\n",
        "[[ -4.9044833   19.29796262]] 0.909090909091\n",
        "[[ -2.81177422  14.87988105]] 0.939393939394\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "On Your Own"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Import the affair data in the file \"affair_dataframe.pkl\". Use `pd.read_pickle(string)` to import it.\n",
      "2. Explore the data available around affairs, determining what seems to more related or correlated to \"affair\"\n",
      "3. Create a logistic model that explains how variables predict the target variable \"affair.\" Remember to use a cross validation technique!\n",
      "4. **\\*** practice the regularization methods. Given a test train split, at what point do you get a good model? Evaluate using accuracy.\n",
      "5. **\\*\\*** How do results change when using k-fold? How would you go about finding the best value k for k-fold?\n",
      "6. **\\*\\*** Compare your results with the model found in \"affairs.pkl\" (load using `import pickle; model = pickle.load(open(filename))`)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Review, Practice, Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Mathematics\n",
      "* [Great review on Linear Algebra](http://cs229.stanford.edu/section/cs229-linalg.pdf)\n",
      "* [Deck on Regularization](http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf)\n",
      "* [Tuning the alpha hyperparameter](http://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
      "\n",
      "#### Machine Learning\n",
      "[Early learning on its application to linear models](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf)\n",
      "\n",
      "#### Code\n",
      "[A Matrix Class implemented in pure python](http://code.activestate.com/recipes/189971-basic-linear-algebra-matrix/)\n",
      "\n",
      "#### Logistic Regression\n",
      "* For more on logistic regression, watch the [first three videos](https://www.youtube.com/playlist?list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE) (30 minutes total) from Chapter 4 of An Introduction to Statistical Learning.\n",
      "* UCLA's IDRE has a handy table to help you remember the [relationship between probability, odds, and log-odds](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm).\n",
      "* Better Explained has a very friendly introduction (with lots of examples) to the [intuition behind \"e\"](http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/).\n",
      "* Here are some useful lecture notes on [interpreting logistic regression coefficients](http://www.unm.edu/~schrader/biostat/bio2/Spr06/lec11.pdf).\n",
      "* yhat has a great tutorial on logistic regression using [statsmodels](http://blog.yhathq.com/posts/logistic-regression-and-python.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}